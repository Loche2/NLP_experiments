{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirror.baidu.com/pypi/simple\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --user paddlenlp==2.5.2 -i https://mirror.baidu.com/pypi/simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.layers import LinearChainCrf, ViterbiDecoder, LinearChainCrfLoss\n",
    "from paddlenlp.metrics import ChunkEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 下载并解压数据集\n",
    "from paddle.utils.download import get_path_from_url\n",
    "URL = \"https://paddlenlp.bj.bcebos.com/paddlenlp/datasets/waybill.tar.gz\"\n",
    "get_path_from_url(URL,\"./\")\n",
    "\n",
    "for i, line in enumerate(open('data/train.txt')):\n",
    "    if 0 < i < 5:\n",
    "        print ('%d: ' % i, line.split()[0])\n",
    "        print ('   ', line.split()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**继承`paddle.io.Dataset`自定义数据集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_tokens_to_ids(tokens, vocab, oov_token=None):\n",
    "    token_ids = []\n",
    "    oov_id = vocab.get(oov_token) if oov_token else None\n",
    "    for token in tokens:\n",
    "        token_id = vocab.get(token, oov_id)\n",
    "        token_ids.append(token_id)\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "def load_dict(dict_path):\n",
    "    vocab = {}\n",
    "    i = 0\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        key = line.strip('\\n')\n",
    "        vocab[key] = i\n",
    "        i += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def load_dataset(datafiles):\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            next(fp)\n",
    "            for line in fp.readlines():\n",
    "                words, labels = line.strip('\\n').split('\\t')\n",
    "                words = words.split('\\002')\n",
    "                labels = labels.split('\\002')\n",
    "                yield words, labels\n",
    "\n",
    "    if isinstance(datafiles, str):\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "train_ds, dev_ds, test_ds = load_dataset(datafiles=('data/train.txt', 'data/dev.txt', 'data/test.txt'))\n",
    "\n",
    "label_vocab = load_dict('./data/tag.dic')\n",
    "word_vocab = load_dict('./data/word.dic')\n",
    "\n",
    "def convert_example(example):\n",
    "        tokens, labels = example\n",
    "        token_ids = convert_tokens_to_ids(tokens, word_vocab, 'OOV')\n",
    "        label_ids = convert_tokens_to_ids(labels, label_vocab, 'O')\n",
    "        return token_ids, len(token_ids), label_ids\n",
    "\n",
    "train_ds.map(convert_example)\n",
    "dev_ds.map(convert_example)\n",
    "test_ds.map(convert_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 构造dataloder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=word_vocab.get('OOV')),  # token_ids\n",
    "        Stack(),  # seq_len\n",
    "        Pad(axis=0, pad_val=label_vocab.get('O'))  # label_ids\n",
    "    ): fn(samples)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "        dataset=train_ds,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "        return_list=True,\n",
    "        collate_fn=batchify_fn)\n",
    "\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "        dataset=dev_ds,\n",
    "        batch_size=32,\n",
    "        drop_last=True,\n",
    "        return_list=True,\n",
    "        collate_fn=batchify_fn)\n",
    "\n",
    "test_loader = paddle.io.DataLoader(\n",
    "        dataset=test_ds,\n",
    "        batch_size=32,\n",
    "        drop_last=True,\n",
    "        return_list=True,\n",
    "        collate_fn=batchify_fn)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 网络构建\n"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BiGRUWithCRF(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 hidden_size,\n",
    "                 word_num,\n",
    "                 label_num,\n",
    "                 use_w2v_emb=False):\n",
    "        super(BiGRUWithCRF, self).__init__()\n",
    "        if use_w2v_emb:\n",
    "            self.word_emb = TokenEmbedding(\n",
    "                extended_vocab_path='./conf/word.dic', unknown_token='OOV')\n",
    "        else:\n",
    "            self.word_emb = nn.Embedding(word_num, emb_size)\n",
    "        self.gru = nn.GRU(emb_size,\n",
    "                          hidden_size,\n",
    "                          num_layers=2,\n",
    "                          direction='bidirectional')\n",
    "        self.fc = nn.Linear(hidden_size * 2, label_num + 2)  # BOS EOS\n",
    "        self.crf = LinearChainCrf(label_num)\n",
    "        self.decoder = ViterbiDecoder(self.crf.transitions)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        embs = self.word_emb(x)\n",
    "        output, _ = self.gru(embs)\n",
    "        output = self.fc(output)\n",
    "        _, pred = self.decoder(output, lens)\n",
    "        return output, lens, pred\n",
    "\n",
    "# Define the model netword and its loss\n",
    "network = BiGRUWithCRF(300, 300, len(word_vocab), len(label_vocab))\n",
    "model = paddle.Model(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 网络配置\n",
    "\n",
    "定义网络结构后，需要配置优化器、损失函数、评价指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizer = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\n",
    "crf_loss = LinearChainCrfLoss(network.crf)\n",
    "chunk_evaluator = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "model.prepare(optimizer, crf_loss, chunk_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": "## 模型训练"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_data=train_loader,\n",
    "              eval_data=dev_loader,\n",
    "              epochs=1,\n",
    "              save_dir='./results',\n",
    "              log_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 模型评估\n",
    "调用`model.evaluate`，查看序列化标注模型在测试集（test.txt）上的评测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(eval_data=test_loader, log_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 预测\n",
    "\n",
    "利用已有模型，可在未知label的数据集（此处复用测试集test.txt）上进行预测，得到模型预测结果及各label的概率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_decodes(ds, decodes, lens, label_vocab):\n",
    "    decodes = [x for batch in decodes for x in batch]\n",
    "    lens = [x for batch in lens for x in batch]\n",
    "    id_label = dict(zip(label_vocab.values(), label_vocab.keys()))\n",
    "\n",
    "    outputs = []\n",
    "    for idx, end in enumerate(lens):\n",
    "        sent = ds.data[idx][0][:end]\n",
    "        tags = [id_label[x] for x in decodes[idx][:end]]\n",
    "        sent_out = []\n",
    "        tags_out = []\n",
    "        words = \"\"\n",
    "        for s, t in zip(sent, tags):\n",
    "            if t.endswith('-B') or t == 'O':\n",
    "                if len(words):\n",
    "                    sent_out.append(words)\n",
    "                tags_out.append(t.split('-')[0])\n",
    "                words = s\n",
    "            else:\n",
    "                words += s\n",
    "        if len(sent_out) < len(tags_out):\n",
    "            sent_out.append(words)\n",
    "        outputs.append(''.join(\n",
    "            [str((s, t)) for s, t in zip(sent_out, tags_out)]))\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs, lens, decodes = model.predict(test_data=test_loader)\n",
    "preds = parse_decodes(test_ds, decodes, lens, label_vocab)\n",
    "\n",
    "print('\\n'.join(preds[:5]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 优化-使用预训练的词向量优化模型效果\n",
    "在Baseline版本中，我们调用了`paddle.nn.Embedding`获取词的向量表示   \n",
    "这里，我们调用`paddlenlp.embeddings`中内置的向量表示`TokenEmbedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.embeddings import TokenEmbedding # EMB\n",
    "\n",
    "del model\n",
    "del preds\n",
    "del network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BiGRUWithCRF2(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 hidden_size,\n",
    "                 word_num,\n",
    "                 label_num,\n",
    "                 use_w2v_emb=True):\n",
    "        super(BiGRUWithCRF2, self).__init__()\n",
    "        if use_w2v_emb:\n",
    "            self.word_emb = TokenEmbedding(\n",
    "                extended_vocab_path='./data/word.dic', unknown_token='OOV')\n",
    "        else:\n",
    "            self.word_emb = nn.Embedding(word_num, emb_size)\n",
    "        self.gru = nn.GRU(emb_size,\n",
    "                          hidden_size,\n",
    "                          num_layers=2,\n",
    "                          direction='bidirectional')\n",
    "        self.fc = nn.Linear(hidden_size * 2, label_num + 2)  # BOS EOS\n",
    "        self.crf = LinearChainCrf(label_num)\n",
    "        self.decoder = ViterbiDecoder(self.crf.transitions)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        embs = self.word_emb(x)\n",
    "        output, _ = self.gru(embs)\n",
    "        output = self.fc(output)\n",
    "        _, pred = self.decoder(output, lens)\n",
    "        return output, lens, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "network = BiGRUWithCRF2(300, 300, len(word_vocab), len(label_vocab))\n",
    "model = paddle.Model(network)\n",
    "optimizer = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\n",
    "crf_loss = LinearChainCrfLoss(network.crf)\n",
    "chunk_evaluator = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "model.prepare(optimizer, crf_loss, chunk_evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_data=train_loader,\n",
    "            eval_data=dev_loader,\n",
    "            epochs=2,\n",
    "            save_dir='./results',\n",
    "            log_freq=1)\n",
    "\n",
    "model.evaluate(eval_data=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.evaluate(eval_data=test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "模型在验证集上的f1 score较之前有明显提升。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outputs, lens, decodes = model.predict(test_data=test_loader)\n",
    "preds = parse_decodes(test_ds, decodes, lens, label_vocab)\n",
    "\n",
    "print('\\n'.join(preds[:5]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

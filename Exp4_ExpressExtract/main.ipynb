{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 基于Bi-GRU+CRF的快递单信息抽取",
   "id": "96283d0d490e6b04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T09:35:23.256344Z",
     "start_time": "2024-06-07T09:35:18.923292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import paddle\n",
    "import paddle.nn as nn\n",
    "\n",
    "import paddlenlp\n",
    "from paddlenlp.datasets import MapDataset\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from paddlenlp.layers import LinearChainCrf, ViterbiDecoder, LinearChainCrfLoss\n",
    "from paddlenlp.metrics import ChunkEvaluator"
   ],
   "id": "8d2b7cebd57d7c07",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\paddle_2.4.1\\lib\\site-packages\\_distutils_hack\\__init__.py:26: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. 数据准备",
   "id": "e64e123027587a89"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T09:35:23.287994Z",
     "start_time": "2024-06-07T09:35:23.256344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 下载并解压数据集\n",
    "from paddle.utils.download import get_path_from_url\n",
    "URL = \"https://paddlenlp.bj.bcebos.com/paddlenlp/datasets/waybill.tar.gz\"\n",
    "get_path_from_url(URL,\"./\")\n",
    "\n",
    "for i, line in enumerate(open('data/train.txt')):\n",
    "    if 0 < i < 5:\n",
    "        print ('%d: ' % i, line.split()[0])\n",
    "        print ('   ', line.split()[1])"
   ],
   "id": "dc132f066d4a4467",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:  1\u00026\u00026\u00022\u00020\u00022\u00020\u00020\u00020\u00027\u00027\u0002宣\u0002荣\u0002嗣\u0002甘\u0002肃\u0002省\u0002白\u0002银\u0002市\u0002会\u0002宁\u0002县\u0002河\u0002畔\u0002镇\u0002十\u0002字\u0002街\u0002金\u0002海\u0002超\u0002市\u0002西\u0002行\u00025\u00020\u0002米\n",
      "    T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\u0002A1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\n",
      "2:  1\u00023\u00025\u00025\u00022\u00026\u00026\u00024\u00023\u00020\u00027\u0002姜\u0002骏\u0002炜\u0002云\u0002南\u0002省\u0002德\u0002宏\u0002傣\u0002族\u0002景\u0002颇\u0002族\u0002自\u0002治\u0002州\u0002盈\u0002江\u0002县\u0002平\u0002原\u0002镇\u0002蜜\u0002回\u0002路\u0002下\u0002段\n",
      "    T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\u0002P-I\u0002A1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A2-I\u0002A2-I\u0002A2-I\u0002A2-I\u0002A2-I\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\n",
      "3:  内\u0002蒙\u0002古\u0002自\u0002治\u0002区\u0002赤\u0002峰\u0002市\u0002阿\u0002鲁\u0002科\u0002尔\u0002沁\u0002旗\u0002汉\u0002林\u0002西\u0002街\u0002路\u0002南\u00021\u00023\u00027\u00020\u00021\u00020\u00028\u00025\u00023\u00029\u00020\u0002那\u0002峥\n",
      "    A1-B\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A3-I\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\n",
      "4:  广\u0002东\u0002省\u0002梅\u0002州\u0002市\u0002大\u0002埔\u0002县\u0002茶\u0002阳\u0002镇\u0002胜\u0002利\u0002路\u00021\u00023\u00026\u00020\u00021\u00023\u00022\u00028\u00021\u00027\u00023\u0002张\u0002铱\n",
      "    A1-B\u0002A1-I\u0002A1-I\u0002A2-B\u0002A2-I\u0002A2-I\u0002A3-B\u0002A3-I\u0002A3-I\u0002A4-B\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002A4-I\u0002T-B\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002T-I\u0002P-B\u0002P-I\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T09:35:23.335285Z",
     "start_time": "2024-06-07T09:35:23.287994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def convert_tokens_to_ids(tokens, vocab, oov_token=None):\n",
    "    token_ids = []\n",
    "    oov_id = vocab.get(oov_token) if oov_token else None\n",
    "    for token in tokens:\n",
    "        token_id = vocab.get(token, oov_id)\n",
    "        token_ids.append(token_id)\n",
    "    return token_ids\n",
    "\n",
    "\n",
    "def load_dict(dict_path):\n",
    "    vocab = {}\n",
    "    i = 0\n",
    "    for line in open(dict_path, 'r', encoding='utf-8'):\n",
    "        key = line.strip('\\n')\n",
    "        vocab[key] = i\n",
    "        i += 1\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def load_dataset(datafiles):\n",
    "    def read(data_path):\n",
    "        with open(data_path, 'r', encoding='utf-8') as fp:\n",
    "            next(fp)\n",
    "            for line in fp.readlines():\n",
    "                words, labels = line.strip('\\n').split('\\t')\n",
    "                words = words.split('\\002')\n",
    "                labels = labels.split('\\002')\n",
    "                yield words, labels\n",
    "\n",
    "    if isinstance(datafiles, str):\n",
    "        return MapDataset(list(read(datafiles)))\n",
    "    elif isinstance(datafiles, list) or isinstance(datafiles, tuple):\n",
    "        return [MapDataset(list(read(datafile))) for datafile in datafiles]\n",
    "\n",
    "train_ds, dev_ds, test_ds = load_dataset(datafiles=('data/train.txt', 'data/dev.txt', 'data/test.txt'))\n",
    "\n",
    "label_vocab = load_dict('./data/tag.dic')\n",
    "word_vocab = load_dict('./data/word.dic')\n",
    "\n",
    "def convert_example(example):\n",
    "    tokens, labels = example\n",
    "    token_ids = convert_tokens_to_ids(tokens, word_vocab, 'OOV')\n",
    "    label_ids = convert_tokens_to_ids(labels, label_vocab, 'O')\n",
    "    return token_ids, len(token_ids), label_ids\n",
    "\n",
    "train_ds.map(convert_example)\n",
    "dev_ds.map(convert_example)\n",
    "test_ds.map(convert_example)"
   ],
   "id": "427ff49e03c63089",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<paddlenlp.datasets.dataset.MapDataset at 0x1c9d2015df0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T09:35:23.351302Z",
     "start_time": "2024-06-07T09:35:23.335285Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=word_vocab.get('OOV')),  # token_ids\n",
    "    Stack(),  # seq_len\n",
    "    Pad(axis=0, pad_val=label_vocab.get('O'))  # label_ids\n",
    "): fn(samples)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "\n",
    "dev_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_size=32,\n",
    "    drop_last=True,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)\n",
    "\n",
    "test_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_size=32,\n",
    "    drop_last=True,\n",
    "    return_list=True,\n",
    "    collate_fn=batchify_fn)"
   ],
   "id": "cd56f4cd443996af",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. 网络构建",
   "id": "150476942dc24a99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T09:35:23.721262Z",
     "start_time": "2024-06-07T09:35:23.351302Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiGRUWithCRF(nn.Layer):\n",
    "    def __init__(self,\n",
    "                 emb_size,\n",
    "                 hidden_size,\n",
    "                 word_num,\n",
    "                 label_num,\n",
    "                 use_w2v_emb=False):\n",
    "        super(BiGRUWithCRF, self).__init__()\n",
    "        if use_w2v_emb:\n",
    "            self.word_emb = TokenEmbedding(\n",
    "                extended_vocab_path='./conf/word.dic', unknown_token='OOV')\n",
    "        else:\n",
    "            self.word_emb = nn.Embedding(word_num, emb_size)\n",
    "        self.gru = nn.GRU(emb_size,\n",
    "                          hidden_size,\n",
    "                          num_layers=2,\n",
    "                          direction='bidirectional')\n",
    "        self.fc = nn.Linear(hidden_size * 2, label_num + 2)  # BOS EOS\n",
    "        self.crf = LinearChainCrf(label_num)\n",
    "        self.decoder = ViterbiDecoder(self.crf.transitions)\n",
    "\n",
    "    def forward(self, x, lens):\n",
    "        embs = self.word_emb(x)\n",
    "        output, _ = self.gru(embs)\n",
    "        output = self.fc(output)\n",
    "        _, pred = self.decoder(output, lens)\n",
    "        return output, lens, pred\n",
    "\n",
    "# Define the model netword and its loss\n",
    "network = BiGRUWithCRF(300, 300, len(word_vocab), len(label_vocab))\n",
    "model = paddle.Model(network)"
   ],
   "id": "bd13ef304b715402",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T09:35:23.736895Z",
     "start_time": "2024-06-07T09:35:23.721262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())\n",
    "crf_loss = LinearChainCrfLoss(network.crf)\n",
    "chunk_evaluator = ChunkEvaluator(label_list=label_vocab.keys(), suffix=True)\n",
    "model.prepare(optimizer, crf_loss, chunk_evaluator)"
   ],
   "id": "aac46db3ce486f63",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. 模型训练",
   "id": "7ef603a8a75f684d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-07T09:35:26.589117Z",
     "start_time": "2024-06-07T09:35:23.736895Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.fit(train_data=train_loader,\n",
    "          eval_data=dev_loader,\n",
    "          epochs=1,\n",
    "          save_dir='./results',\n",
    "          log_freq=1)"
   ],
   "id": "bd9c477a9835fec1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[33m[2024-06-07 17:35:23,915] [ WARNING]\u001B[0m - Compatibility Warning: The params of LinearChainCrfLoss.forward has been modified. The third param is `labels`, and the fourth is not necessary. Please update the usage.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The loss value printed in the log is the current step, and the metric is the average value of previous steps.\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "(InvalidArgument) The type of data we are trying to retrieve does not match the type of data currently contained in the container.\n  [Hint: Expected dtype() == paddle::experimental::CppTypeToDataType<T>::Type(), but received dtype():7 != paddle::experimental::CppTypeToDataType<T>::Type():9.] (at ..\\paddle\\phi\\core\\dense_tensor.cc:143)\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      2\u001B[0m \u001B[43m          \u001B[49m\u001B[43meval_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdev_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m          \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m          \u001B[49m\u001B[43msave_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m./results\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m          \u001B[49m\u001B[43mlog_freq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\paddle_2.4.1\\lib\\site-packages\\paddle\\hapi\\model.py:1781\u001B[0m, in \u001B[0;36mModel.fit\u001B[1;34m(self, train_data, eval_data, batch_size, epochs, eval_freq, log_freq, save_dir, save_freq, verbose, drop_last, shuffle, num_workers, callbacks, accumulate_grad_batches, num_iters)\u001B[0m\n\u001B[0;32m   1779\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(epochs):\n\u001B[0;32m   1780\u001B[0m     cbks\u001B[38;5;241m.\u001B[39mon_epoch_begin(epoch)\n\u001B[1;32m-> 1781\u001B[0m     logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_one_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcbks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1782\u001B[0m     cbks\u001B[38;5;241m.\u001B[39mon_epoch_end(epoch, logs)\n\u001B[0;32m   1784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m do_eval \u001B[38;5;129;01mand\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m eval_freq \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\paddle_2.4.1\\lib\\site-packages\\paddle\\hapi\\model.py:2112\u001B[0m, in \u001B[0;36mModel._run_one_epoch\u001B[1;34m(self, data_loader, callbacks, mode, logs)\u001B[0m\n\u001B[0;32m   2108\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mode \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m   2109\u001B[0m     _inputs\u001B[38;5;241m.\u001B[39mappend((step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m%\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_accumulate \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m   2110\u001B[0m                    \u001B[38;5;129;01mor\u001B[39;00m step \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(data_loader))\n\u001B[1;32m-> 2112\u001B[0m outs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43m_batch\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m_inputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2114\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_metrics \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_loss:\n\u001B[0;32m   2115\u001B[0m     metrics \u001B[38;5;241m=\u001B[39m [[l[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mfor\u001B[39;00m l \u001B[38;5;129;01min\u001B[39;00m outs[\u001B[38;5;241m0\u001B[39m]]]\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\paddle_2.4.1\\lib\\site-packages\\paddle\\hapi\\model.py:1102\u001B[0m, in \u001B[0;36mModel.train_batch\u001B[1;34m(self, inputs, labels, update)\u001B[0m\n\u001B[0;32m   1054\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtrain_batch\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, labels\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, update\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m   1055\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1056\u001B[0m \u001B[38;5;124;03m    Run one training step on one batch of data. And using `update` indicates\u001B[39;00m\n\u001B[0;32m   1057\u001B[0m \u001B[38;5;124;03m    whether optimizer update gradients computing by this batch.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;124;03m            # [array([2.192784], dtype=float32)]\u001B[39;00m\n\u001B[0;32m   1101\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1102\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_adapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_batch\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mupdate\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1103\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fluid\u001B[38;5;241m.\u001B[39m_non_static_mode() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_input_info \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1104\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inputs()\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\paddle_2.4.1\\lib\\site-packages\\paddle\\hapi\\model.py:727\u001B[0m, in \u001B[0;36mDynamicGraphAdapter.train_batch\u001B[1;34m(self, inputs, labels, update)\u001B[0m\n\u001B[0;32m    724\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    725\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39mnetwork(\u001B[38;5;241m*\u001B[39m[to_variable(x) \u001B[38;5;28;01mfor\u001B[39;00m x \u001B[38;5;129;01min\u001B[39;00m inputs])\n\u001B[1;32m--> 727\u001B[0m losses \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mto_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutputs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    728\u001B[0m losses \u001B[38;5;241m=\u001B[39m to_list(losses)\n\u001B[0;32m    729\u001B[0m final_loss \u001B[38;5;241m=\u001B[39m fluid\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39msum(losses)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\paddle_2.4.1\\lib\\site-packages\\paddle\\fluid\\dygraph\\layers.py:948\u001B[0m, in \u001B[0;36mLayer.__call__\u001B[1;34m(self, *inputs, **kwargs)\u001B[0m\n\u001B[0;32m    945\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m in_declarative_mode()) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks) \\\n\u001B[0;32m    946\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_post_hooks) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_built) \u001B[38;5;129;01mand\u001B[39;00m in_dygraph_mode() \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;129;01mnot\u001B[39;00m in_profiler_mode()):\n\u001B[0;32m    947\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_once(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m--> 948\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    949\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    950\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dygraph_call_func(\u001B[38;5;241m*\u001B[39minputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\paddle_2.4.1\\lib\\site-packages\\paddlenlp\\layers\\crf.py:281\u001B[0m, in \u001B[0;36mLinearChainCrfLoss.forward\u001B[1;34m(self, inputs, lengths, labels, old_version_labels)\u001B[0m\n\u001B[0;32m    277\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning(\n\u001B[0;32m    278\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCompatibility Warning: The params of LinearChainCrfLoss.forward has been modified. The third param is `labels`, and the fourth is not necessary. Please update the usage.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    279\u001B[0m         )\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhas_warn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m--> 281\u001B[0m loss \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcrf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlengths\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcrf\u001B[38;5;241m.\u001B[39mgold_score(inputs, labels, lengths))\n\u001B[0;32m    282\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\paddle_2.4.1\\lib\\site-packages\\paddlenlp\\layers\\crf.py:134\u001B[0m, in \u001B[0;36mLinearChainCrf.forward\u001B[1;34m(self, inputs, lengths)\u001B[0m\n\u001B[0;32m    132\u001B[0m batch_index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_batch_index(batch_size)\n\u001B[0;32m    133\u001B[0m last_index \u001B[38;5;241m=\u001B[39m lengths \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m--> 134\u001B[0m idxs \u001B[38;5;241m=\u001B[39m \u001B[43mpaddle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mbatch_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlast_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    135\u001B[0m alpha \u001B[38;5;241m=\u001B[39m paddle\u001B[38;5;241m.\u001B[39mgather_nd(all_alpha, idxs)\n\u001B[0;32m    137\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_start_stop_tag:\n\u001B[0;32m    138\u001B[0m     \u001B[38;5;66;03m# The last one step\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Anaconda3\\envs\\paddle_2.4.1\\lib\\site-packages\\paddle\\tensor\\manipulation.py:1839\u001B[0m, in \u001B[0;36mstack\u001B[1;34m(x, axis, name)\u001B[0m\n\u001B[0;32m   1836\u001B[0m axis \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m axis \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m axis\n\u001B[0;32m   1838\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m in_dygraph_mode():\n\u001B[1;32m-> 1839\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_C_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstack\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1841\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _in_legacy_dygraph():\n\u001B[0;32m   1842\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _legacy_C_ops\u001B[38;5;241m.\u001B[39mstack(x, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124maxis\u001B[39m\u001B[38;5;124m'\u001B[39m, axis)\n",
      "\u001B[1;31mValueError\u001B[0m: (InvalidArgument) The type of data we are trying to retrieve does not match the type of data currently contained in the container.\n  [Hint: Expected dtype() == paddle::experimental::CppTypeToDataType<T>::Type(), but received dtype():7 != paddle::experimental::CppTypeToDataType<T>::Type():9.] (at ..\\paddle\\phi\\core\\dense_tensor.cc:143)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. 模型评估",
   "id": "c0d92b6962af02be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "model.evaluate(eval_data=test_loader, log_freq=1)",
   "id": "c8c7e50f5037e8c3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
